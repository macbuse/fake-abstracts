{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = 'JOURNO/'\n",
    "fns = [path + x for x in os.listdir(path) if x[:2] == 'jj']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bits = []\n",
    "for fn in fns[:]:\n",
    "    with open(fn,'r') as fp:\n",
    "        dd = [fp.readline().strip() for x in range(3)]\n",
    "    bits.append((dd[0],dd[2]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I scraped articles to a txt file, the format is as follows\n",
    "\n",
    "- line 0 = title\n",
    "- blank\n",
    "- line 2 = contents of the meta i.e. kwords\n",
    "- blank\n",
    "- contents\n",
    "- blank\n",
    "- line -1 = src URL\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles, kwords = zip(* bits)\n",
    "vocab = ','.join(kwords).split(',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the format one can get the contents as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "arts  = []\n",
    "for i in bag:\n",
    "    with open(fns[i],'r') as fp:\n",
    "        dd = ''.join(fp.readlines()[4:-2]).replace('\\n', ' ')\n",
    "      \n",
    "    arts.append(dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(789779, 164)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "lengths = [len(x) for x in arts]\n",
    "totals = [(sum(lengths[:i]),i)  for i,x in enumerate(lengths)]\n",
    "totals[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx = ' '.join(arts)\n",
    "with open('text6.txt', 'w') as fp:\n",
    "    fp.write(tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "word_count = Counter(vocab)\n",
    "wc = sorted(list(word_count.items()), key = lambda x : -x[1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "junk = ','.join(word_count.keys() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = textacy.doc.Doc(junk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = list(textacy.extract.named_entities(doc, drop_determiners=True, exclude_types='numeric'))\n",
    "\n",
    "people = [x for x in names if x.label_ == 'PERSON' and len(x) > 1]\n",
    "people[0].label_ == 'ORG'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Julian Assange', 2)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people[0].text, len(people[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Models,Bestival,Photography,Cameron,Royal,Galloway,Men,Burberry,Cameron,Ant,Tights,Parks,Rosetta,Zara,Seinfeld,Older,Drama,Pornography,Rihanna,Royal,Biography,Burkini,Theresa,Depp,Slimane,Law'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "','.join([x.text for x in people if len(x) < 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "others = 'Cameron,Royal,Galloway,Cameron,Seinfeld,Rihanna,Theresa,Depp,Slimane'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "persons = [ x.text for x in people]\n",
    "persons.extend(others.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Seinfeld', 'Rihanna', 'Theresa', 'Depp', 'Slimane']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag = []\n",
    "\n",
    "for i, kk in enumerate(kwords):\n",
    "    for x in persons:\n",
    "        if x in kk : \n",
    "            bag.append(i)\n",
    "            break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I read somewhere that Blake Lively – who I guess is a person and not an exotic fruit – is to play a young Carrie Bradshaw. Didn\\'t you say that the Sex and the City films had killed the franchise? How is this being allowed?  Mariella, I was as shocked as you. As you say, I made that ringing decree and read the last rites over the flailing corpse of Sex and the City and the series producer, Michael Patrick King, has wilfully – nay, foolishly – disregarded my command and is said to be planning a prequel to the series in which Ms Lively plays a young Carrie. Ah, prequels: sometimes good in theory, never good in practice. George Lucas, what are your thoughts on the matter?  While we await George\\'s opinion, I need to stress two things: one, I personally think Blake Lively sounds more like the colour of a paint than an exotic fruit (\"Yes, I like the idea of green for the kitchen but I don\\'t want one that\\'s too blake lively\"). And two, this is a story that came from a terribly trashy magazine that does not, shall we say, adhere to New Yorker magazine levels of fact-checking. But what the hey, Mariella – this story caught my eye, too, and so let\\'s follow the example of the terribly trashy magazine and not let pesky things such as journalistic truth get in the way of an article.  This is clearly not just a bad idea, but a terrible idea. It is a remarkable truth that the longer Sex and the City went on, and the more successful it got, the stupider everyone involved in it appeared to become. The TV show might have had its occasional faults (not least the \"happy\" ending in which Carrie got together with a man who had always treated her like crap. Ding dong my prince has come!). But this represented Andrea Dworkin-levels of feminism compared with the films, which appeared to have been written by someone as an illustration of the evils of capitalism and the western world, and the stupidity of women. And yet, surprisingly, it wasn\\'t Osama bin Laden\\'s name on the credits, but King and Sarah Jessica Parker.  The mystery of how two people who created something as great as the first three series of the show could then knock out that second film is as puzzling as how Jennifer Grey could so mutilate her magnificent nose by plastic surgery into such career-destroying anonymity.  The films suggested that King thought the appeal of the shows was the shopping and the sex when it was the humour and the friendships. Lively is the star of Gossip Girl, a TV show that features little but shopping and sex. If King really has signed Lively for this role, do you see what he did there? Are you getting my point? Do I need to hammer it home as heavily as the jokes were hammered by the characters in the second movie INTO YOUR SKULL?  So, as this paper\\'s Sex and the City correspondent (they offered me the Middle East, but I know where my talents are needed most), I shall make a final ringing decree: if this tale really is true then clearly King\\'s descent into idiocy is complete. Now excuse me while I go get my shovel to chuck the first lot of dirt on SATC\\'s coffin.  As fascinated as I am by the whole WikiLeaks story, I cannot follow any of it because I am so distracted by one overwhelming question: has Julian Assange made it trendy to dye one\\'s hair grey?  As fascinated as I am by your query, Philip, I am distracted by another question: is Julian Assange the male Kate Moss? Does his Midas touch make things trendy? Are allegedly torn condoms this year\\'s Fendi baguette. No, I say! No, no and thrice nay!  In fact, grey hair was becoming a touch – to use your adorable daddy-o word – trendy, PA (pre-Assange). Fashion blogger Tavi dyed her hair grey, you know. A journalist at Vogue wrote about having grey hair. And, oh, someone else, probably. Trendy!  But now that pesky Australian has emerged from the underground (where, incidentally, he doesn\\'t seem to spend too much time. Seriously, I am more undercover than Julian Assange), looking as if he has been doing, well, something to his hair inbetween attempting to rock governments.  The bad news is this has made dyeing one\\'s hair grey very untrendy. The good news is it might discourage any man out there from dyeing their hair which, as we have discussed previously, is never ever a good thing. '"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = textacy.preprocess_text(arts[0], lowercase=False, no_punct=True)\n",
    "doc = textacy.doc.Doc(arts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sarah Jessica Parker',\n",
       " 'Julian Assange',\n",
       " 'King',\n",
       " 'Gossip Girl',\n",
       " 'George',\n",
       " 'Kate Moss',\n",
       " 'Mariella',\n",
       " 'Carrie Bradshaw',\n",
       " \"Osama bin Laden's\",\n",
       " 'Blake Lively – who',\n",
       " 'Ding',\n",
       " 'Philip',\n",
       " 'Midas',\n",
       " 'Michael Patrick King',\n",
       " 'Andrea Dworkin-levels',\n",
       " 'Jennifer Grey',\n",
       " 'Carrie',\n",
       " 'Blake Lively',\n",
       " 'Tavi',\n",
       " 'George Lucas']"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = list(textacy.extract.named_entities(doc, drop_determiners=True, exclude_types='numeric'))\n",
    "names = list(set(names))\n",
    "names.sort(key = lambda x : -len(x))\n",
    "\n",
    "nearly_unique = [ x.text for x in names if x.label_ == 'PERSON']\n",
    "#monikers = [x for x in nearly_unique if ' ' not in x]\n",
    "#set(nearly_unique)\n",
    "nearly_unique = list(set(nearly_unique))\n",
    "nearly_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replacement strategy\n",
    "\n",
    "This is a greedy strategy.\n",
    "\n",
    "1. try and replace the biggest string first\n",
    "1. replace the substrings which should be forename, last name\n",
    "1. finally look at what is left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacers = []\n",
    "for  x in nearly_unique:\n",
    "    tt = x.split(' ')\n",
    "    nn = len(tt) \n",
    "    if nn > 1:\n",
    "        tt = x, x.split(' ')[0], x.split(' ')[-1]\n",
    "    replacers.append(tt)\n",
    "    \n",
    "big_replacers = [x for x in replacers if len(x) > 1]\n",
    "little_replacers = [x for x in replacers if len(x) == 1]\n",
    "\n",
    "txt = arts[0]\n",
    "for i, x in enumerate(big_replacers):\n",
    "    for ss in x:\n",
    "        txt = txt.replace(ss,'PERSON%d'%i)\n",
    "        \n",
    "total = i\n",
    "for i, ss in enumerate(little_replacers):\n",
    "\n",
    "    txt = txt.replace(ss[0],'PERSON%d'%(i+total) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll deal with the other entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacers2 = []\n",
    "cats = ['WORK_OF_ART', 'ORG', 'GPE', 'PRODUCT']\n",
    "tots = {x: 0 for x in cats}\n",
    "for x in names:\n",
    "    if x.label_ in cats:\n",
    "        payload = x.text, '%s%d'%(x.label_, tots[x.label_])\n",
    "        tots[x.label_] += 1\n",
    "        replacers2.append(payload) \n",
    "        \n",
    "for a,b in replacers2:\n",
    "    txt = txt.replace(a,b )       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trash = []\n",
    "\n",
    "for base_txt in arts[:]:\n",
    "\n",
    "    doc = textacy.doc.Doc(base_txt)\n",
    "    txt = base_txt[:]\n",
    "\n",
    "    names = list(textacy.extract.named_entities(doc, drop_determiners=True, exclude_types='numeric'))\n",
    "    names = list(set(names))\n",
    "    names.sort(key = lambda x : -len(x))\n",
    "\n",
    "    nearly_unique = [ x.text for x in names if x.label_ == 'PERSON']\n",
    "\n",
    "\n",
    "    replacers = []\n",
    "    for  x in list(set(nearly_unique)):\n",
    "        tt = x.split(' ')\n",
    "        nn = len(tt) \n",
    "        if nn > 1:\n",
    "            tt = x, x.split(' ')[0], x.split(' ')[-1]\n",
    "        replacers.append(tt)\n",
    "\n",
    "    big_replacers = [x for x in replacers if len(x) > 1]\n",
    "    little_replacers = [x for x in replacers if len(x) == 1]\n",
    "\n",
    "    # for some reason there was 'in' in the repacement queue\n",
    "    for i, x in enumerate(big_replacers):\n",
    "        for ss in x:\n",
    "            if len(ss) < 3: continue\n",
    "            txt = txt.replace(ss,'PERSON%d'%i)\n",
    "\n",
    "    total = i\n",
    "    for i, ss in enumerate(little_replacers):\n",
    "        if len(ss) < 3: continue\n",
    "        txt = txt.replace(ss[0],'PERSON%d'%(i+total) )\n",
    "\n",
    "\n",
    "    replacers2 = []\n",
    "    cats = ['WORK_OF_ART', 'ORG', 'GPE', 'PRODUCT']\n",
    "    tots = {x: 0 for x in cats}\n",
    "    for x in names:\n",
    "        #need to fix this\n",
    "        if len(x.text) < 3: continue\n",
    "        if x.label_ in cats:\n",
    "            payload = x.text, '%s%d'%(x.label_, tots[x.label_])\n",
    "            tots[x.label_] += 1\n",
    "            replacers2.append(payload) \n",
    "\n",
    "    for a,b in replacers2:\n",
    "        txt = txt.replace(a,b ) \n",
    "    \n",
    "    trash.append(txt)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trash[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLPkernel",
   "language": "python",
   "name": "nlpkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
